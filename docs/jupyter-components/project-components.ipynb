{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component One: Voice Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice\n",
    "from scipy.io.wavfile import write\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Record Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component will record an audio sample, and save it to an audio file.\n",
    "\n",
    "### Process\n",
    "1. Run the cell below\n",
    "2. Input a number for the amount of time you want the recorder to run\n",
    "3. (Maybe) grant access for the use of your devices microphone\n",
    "4. Speak\n",
    "5. The cell will automatically stop running after the time is completed\n",
    "\n",
    "### Details\n",
    "- The number of channels is specific for your device\n",
    "- The function creates a new file or rewrites the existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording.....n\n",
      "Finished.....nPlease check your output file\n"
     ]
    }
   ],
   "source": [
    "fs= 44100\n",
    "second =  int(input(\"Enter time duration in seconds: \"))\n",
    "print(\"Recording.....n\")\n",
    "record_voice = sounddevice.rec( int ( second * fs ) , samplerate = fs , channels = 1, dtype=np.int16 ) # might be different depending on machine\n",
    "sounddevice.wait()\n",
    "write(\"audio_file.wav\",fs,record_voice)\n",
    "print(\"Finished.....nPlease check your output file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transcribe Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component takes an audio file and writes its content to a text file\n",
    "\n",
    "### Process\n",
    "1. Download the vosk model. The linto french model is suggested\n",
    "2. Run the cell below\n",
    "\n",
    "### Details\n",
    "- the model must be downloaded and have the appropriate name\n",
    "- the file names are provided at the beginning of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| os.getcwd(): '/Users/ryanheadley/epitech/tor_2021_3/docs/jupyter-components'\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:CompileLooped():nnet-compile-looped.cc:345) Spent 0.0880129 seconds in looped compilation.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from ../../app/app/back/nlp/models/linto/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:278) Loading HCLG from ../../app/app/back/nlp/models/linto/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:293) Loading words from ../../app/app/back/nlp/models/linto/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:302) Loading winfo ../../app/app/back/nlp/models/linto/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:309) Loading subtract G.fst model from ../../app/app/back/nlp/models/linto/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:311) Loading CARPA model from ../../app/app/back/nlp/models/linto/rescore/G.carpa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "\n",
    "model_name = \"linto\"\n",
    "audio_file = './audio_file.wav'\n",
    "text_file = 'transcription.txt'\n",
    "\n",
    "model_path = \"../../app/app/back/nlp/models/{}\".format(model_name)\n",
    "ic(os.getcwd())\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Please download the model from https://alphacephei.com/vosk/models and unpack as 'model' in the current folder.\")\n",
    "    exit(1)\n",
    "\n",
    "wf = wave.open(audio_file, \"rb\")\n",
    "\n",
    "if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "    print (\"Audio file must be WAV format mono PCM.\")\n",
    "    exit(1)\n",
    "\n",
    "model = Model(model_path)\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "# rec.SetMaxAlternatives(10)\n",
    "# rec.SetWords(True)\n",
    "\n",
    "result = []\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        result.append(json.loads(rec.Result()))\n",
    "\n",
    "# ret = [sentence[\"alternatives\"][0][\"text\"] for sentence in result]\n",
    "ret = result[0][\"text\"]\n",
    "\n",
    "with open(text_file, 'w') as file:\n",
    "    file.write(ret)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deux aller de paris à lyon en train']\n"
     ]
    }
   ],
   "source": [
    "with open(text_file, 'r') as f:\n",
    "    print(f.readlines())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Two: Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component takes a text file and extracts a travel request destination and departure\n",
    "\n",
    "### Process\n",
    "1. Run the cell below\n",
    "\n",
    "### Details\n",
    "- the model must be downloaded and have the appropriate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'departure': 'paris', 'destination': 'lyon'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = 'transcription.txt'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "words_before_departure = ['de', 'depuis', 'provenance']\n",
    "words_before_destination = ['à', 'a', 'en', 'jusqu\\'a', 'vers', 'par']\n",
    "example_travel_sentence = ['Je veux prendre un train de paris à lyon']\n",
    "\n",
    "def get_cities(sentence):\n",
    "    \"\"\" Take a sentence and return all cities within\n",
    "\n",
    "    Args:\n",
    "        sentence (str): any sentence\n",
    "\n",
    "    Returns:\n",
    "        Array: A list of cities\n",
    "    \"\"\"\n",
    "    cities = []\n",
    "    doc = nlp(sentence)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"LOC\":\n",
    "            cities.append(entity.text)\n",
    "    \n",
    "    return cities\n",
    "\n",
    "def check_for_travel_request(sentences):\n",
    "    \"\"\" Take a list of sentences and return the sentence\n",
    "        containing a request to travel by train\n",
    "\n",
    "    Args:\n",
    "        sentences (Array<str>): List of sentences\n",
    "\n",
    "    Returns:\n",
    "        str: travel request sentence or SPAM\n",
    "    \"\"\"\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    real_sentence_embedding = model.encode(example_travel_sentence)\n",
    "    similarities = cosine_similarity(\n",
    "        [real_sentence_embedding[0]],\n",
    "        sentence_embeddings\n",
    "    )\n",
    "    biggest_number = max(similarities[0])\n",
    "    if biggest_number < 0.75:\n",
    "        return False\n",
    "    best_sentence_ind = np.where(similarities[0] == biggest_number)\n",
    "    return sentences[best_sentence_ind[0][0]]\n",
    "\n",
    "def get_destination_and_departure():\n",
    "    \"\"\"Takes the pre-defined text file and determines \n",
    "    the destination and departure\n",
    "\n",
    "    Returns:\n",
    "        dict: of destination and departure \n",
    "        OR False if text not valid\n",
    "    \"\"\"\n",
    "    # read the text file\n",
    "    file = open(text_file, \"r\")\n",
    "    sentences = file.read()\n",
    "    file.close()\n",
    "    if '.' in sentences:\n",
    "        sentences.split('.')\n",
    "    else:\n",
    "        sentences = [sentences]\n",
    "\n",
    "    if detect(sentences[0]) != 'fr':\n",
    "        return False\n",
    "    \n",
    "    # check for travel request\n",
    "    request = check_for_travel_request(sentences)\n",
    "    if not request:\n",
    "        return False\n",
    "    \n",
    "    # get destination and departure\n",
    "    departure = []\n",
    "    destination = []\n",
    "    \n",
    "    cities = get_cities(request)\n",
    "    words = word_tokenize(request)\n",
    "    for city in cities:\n",
    "        index = words.index(city)\n",
    "        if index == 0: continue\n",
    "        if words[index-1] in words_before_departure: departure.append(city)\n",
    "        elif words[index-1] in words_before_destination: destination.append(city)\n",
    "    \n",
    "    return {\n",
    "        \"departure\": departure[0],\n",
    "        \"destination\": destination[0]\n",
    "    }\n",
    "    \n",
    "journey = get_destination_and_departure()\n",
    "journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Three: Pathfinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component takes a departure and destination, and calculates the fastest path between the nearest train stations to these locations.\n",
    "\n",
    "### Process\n",
    "1. Run the cell below\n",
    "2. Wait until the calulation is done.\n",
    "---\n",
    "\n",
    "### Details\n",
    "1. May take some time to generate the itinary depending on the number of nodes between the **departure** and the **arrival**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from geopy.geocoders import Nominatim\n",
    "import mpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate or load the **train stations** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cities():\n",
    "    cities = None\n",
    "    f = open(\"cities.pkl\", \"rb\")\n",
    "    cities = pickle.load(f)\n",
    "    f.close()\n",
    "    if cities is None:\n",
    "        stops = pd.read_csv('../jupyter-notebook/data/data_sncf/stops.txt', sep=\",\")\n",
    "        stops = stops[stops['stop_id'].str.contains('StopPoint:OCETrain')]\n",
    "        stops = stops.set_index('stop_id').T.to_dict()\n",
    "        cities = {}\n",
    "\n",
    "        for city in list(stops.items()):\n",
    "            cities.update({\n",
    "                city[0]: {\n",
    "                    \"stop_name\": city[1][\"stop_name\"],\n",
    "                    \"coord\": [city[1][\"stop_lat\"], city[1][\"stop_lon\"]],\n",
    "            }})\n",
    "        cities_file = open(\"cities.pkl\", \"wb\")\n",
    "        pickle.dump(cities, cities_file)\n",
    "        cities_file.close()\n",
    "    return cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trips():\n",
    "    trips = None\n",
    "    f = open(\"trips.pkl\", \"rb\")\n",
    "    trips_tmp = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    if trips_tmp is None:\n",
    "        stop_times = pd.read_csv('./../data/data_sncf/stop_times.csv', sep=\",\")\n",
    "        trips = pd.read_csv('./../data/data_sncf/trips.csv', sep=\",\")\n",
    "\n",
    "        stop_times = stop_times[stop_times['stop_id'].str.contains('StopPoint:OCETrain')]\n",
    "\n",
    "        trips = trips.drop(labels=[\"service_id\", \"block_id\", \"shape_id\", \"trip_headsign\"], axis=1)\n",
    "\n",
    "        trips = trips.set_index('trip_id').T.to_dict()\n",
    "\n",
    "        trips_tmp = {}\n",
    "        for trip in list(trips.items()):\n",
    "            trips_tmp.update({trip[0]:{\"nodes\": []}})\n",
    "            selected_stop_times = stop_times.loc[stop_times['trip_id'] == trip[0]]\n",
    "            for trip_tmp in selected_stop_times.iterrows():\n",
    "                trips_tmp[trip[0]][\"nodes\"].append({\"trip_id\": trip[0], \"stop_id\": trip_tmp[1][\"stop_id\"],\"arrival_time\": trip_tmp[1][\"arrival_time\"]})\n",
    "    trips_file = open(\"trips.pkl\", \"wb\")\n",
    "    pickle.dump(trips_tmp, trips_file)\n",
    "    trips_file.close()\n",
    "    return trips_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate or load the **graph**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph():\n",
    "    routes_graph = None\n",
    "    f = open(\"graph.pkl\", \"rb\")\n",
    "    routes_graph = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    trips_tmp = load_trips()\n",
    "    cities = load_cities()\n",
    "\n",
    "    if routes_graph is None:\n",
    "        routes_graph = {}\n",
    "        for route in list(trips_tmp.items()):\n",
    "            for i in range (0, len(route[1][\"nodes\"])):\n",
    "                city_id = route[1][\"nodes\"][i][\"stop_id\"]\n",
    "                city = cities[city_id]\n",
    "                if city_id not in routes_graph:\n",
    "                    routes_graph.update({city_id: []})\n",
    "                    if i != 0:\n",
    "                        routes_graph[city_id].append({route[1][\"nodes\"][i - 1][\"stop_id\"]: get_duration_node_to_node(route[1][\"nodes\"][i - 1][\"arrival_time\"], route[1][\"nodes\"][i][\"arrival_time\"])})\n",
    "                    if i != len(route[1][\"nodes\"]) - 1:\n",
    "                        routes_graph[city_id].append({route[1][\"nodes\"][i + 1][\"stop_id\"]: get_duration_node_to_node(route[1][\"nodes\"][i][\"arrival_time\"], route[1][\"nodes\"][i + 1][\"arrival_time\"])})\n",
    "                elif route[1][\"nodes\"][i - 1][\"stop_id\"] not in routes_graph[city_id] and i != 0:\n",
    "                    routes_graph[city_id].append({route[1][\"nodes\"][i - 1][\"stop_id\"]: get_duration_node_to_node(route[1][\"nodes\"][i - 1][\"arrival_time\"], route[1][\"nodes\"][i][\"arrival_time\"])})\n",
    "                elif i < len(route[1][\"nodes\"]) - 1 and route[1][\"nodes\"][i + 1][\"stop_id\"] not in routes_graph[city_id]:\n",
    "                    routes_graph[city_id].append({route[1][\"nodes\"][i + 1][\"stop_id\"]: get_duration_node_to_node(route[1][\"nodes\"][i][\"arrival_time\"], route[1][\"nodes\"][i + 1][\"arrival_time\"])})\n",
    "    graph_file = open(\"graph.pkl\", \"wb\")\n",
    "    pickle.dump(routes_graph, graph_file)\n",
    "    graph_file.close()\n",
    "    return routes_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Using the Generated Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for **duration** of travel between **two nodes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_node_to_node(origin, dest):\n",
    "    start_minute = int(origin.split(\":\")[1])\n",
    "    start_hour = int(origin.split(\":\")[0])\n",
    "\n",
    "    arrival_minute = int(dest.split(\":\")[1])\n",
    "    arrival_hour = int(dest.split(\":\")[0])\n",
    "\n",
    "    start_delta = timedelta(\n",
    "        seconds=0,\n",
    "        minutes=start_minute,\n",
    "        hours=start_hour,\n",
    "     )\n",
    "\n",
    "    arrival_delta = timedelta(\n",
    "        seconds=0,\n",
    "        minutes=arrival_minute,\n",
    "        hours=arrival_hour,\n",
    "     )\n",
    "\n",
    "    return arrival_delta - start_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=7500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = '23:05:00'\n",
    "s2 = '25:10:00'\n",
    "\n",
    "get_duration_node_to_node(s1,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get **geolocation** of a **city**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geolocation(city):\n",
    "    \"\"\"Get the geolocation of a city\n",
    "\n",
    "    Args:\n",
    "        city (str): name of a city\n",
    "\n",
    "    Returns:\n",
    "        Array<geopy.location.Location>: Name, region, department, country, [lat, long, other]\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"travel_request\")\n",
    "    location = geolocator.geocode(city)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geo_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the distance between to geographical points\n",
    "\n",
    "    Args:\n",
    "        lat_1 (float): first latitude\n",
    "        long_1 (float): first longitude\n",
    "        lat_2 (float): second latitude\n",
    "        long_2 (float): second longitude\n",
    "    \"\"\"\n",
    "    return mpu.haversine_distance((lat1, lon1), (lat2, lon2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the closest **train station** for the given **cities**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_stations(cities):\n",
    "    \"\"\"\n",
    "    Get closest train station for each cities passed\n",
    "    Args:\n",
    "        cities (dict): departure and destination\n",
    "\n",
    "    Returns:\n",
    "        dict: departure station and destination station\n",
    "    \"\"\"\n",
    "    stops = pd.read_csv('../jupyter-notebook/data/data_sncf/stops.txt') # TODO: fix path for application\n",
    "    stops = stops[stops['stop_id'].str.contains('StopPoint:OCETrain')]\n",
    "    stop_times = pd.read_csv('../jupyter-notebook/data/data_sncf/stop_times.txt')\n",
    "    geo_departure = get_geolocation(cities[\"departure\"])\n",
    "    geo_destination = get_geolocation(cities[\"destination\"])\n",
    "\n",
    "    departure = {\n",
    "        \"current_lat\": geo_departure.latitude,\n",
    "        \"current_lon\": geo_departure.longitude,\n",
    "        \"stop\": \"\",\n",
    "        \"distance\": 99999,\n",
    "        \"arrival_time\": 0\n",
    "    }\n",
    "    destination = {\n",
    "        \"current_lat\": geo_destination.latitude,\n",
    "        \"current_lon\": geo_destination.longitude,\n",
    "        \"stop\": \"\",\n",
    "        \"distance\": 99999,\n",
    "        \"arrival_time\": 0\n",
    "    }\n",
    "    for index, row in stops.iterrows():\n",
    "        distance_to_departure = get_geo_distance(\n",
    "            departure[\"current_lat\"],\n",
    "            departure[\"current_lon\"],\n",
    "            row.stop_lat,\n",
    "            row.stop_lon\n",
    "        )\n",
    "        if distance_to_departure < departure[\"distance\"]:\n",
    "            departure[\"stop\"] = row.stop_id\n",
    "            departure[\"distance\"] = distance_to_departure\n",
    "\n",
    "        distance_to_destination = get_geo_distance(\n",
    "            destination[\"current_lat\"],\n",
    "            destination[\"current_lon\"],\n",
    "            row.stop_lat,\n",
    "            row.stop_lon\n",
    "        )\n",
    "        if distance_to_destination < destination[\"distance\"]:\n",
    "            destination[\"stop\"] = row.stop_id\n",
    "            destination[\"distance\"] = distance_to_destination\n",
    "    return {departure[\"stop\"]: timedelta(hours=0)}, {destination[\"stop\"]: timedelta(hours=0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a **route** to a *list* of **train station**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_route_to_cities(route):\n",
    "    cities = load_cities()\n",
    "    route_cities = []\n",
    "    for city in route[\"route\"]:\n",
    "        route_cities.append(cities[list(city)[0]][\"stop_name\"])\n",
    "    return route_cities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a **train_station** to **stop point**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_city_to_stop_points(city):\n",
    "    stops_tmp = pd.read_csv('../jupyter-notebook/data/data_sncf/stops.txt', sep=\",\")\n",
    "    stops_tmp = stops_tmp[stops_tmp['stop_id'].str.contains('StopPoint:OCETrain')]\n",
    "    stops_tmp = stops_tmp[stops_tmp['stop_name'].str.contains(city)]\n",
    "    stop_points = []\n",
    "    for stop in stops_tmp.iterrows():\n",
    "        stop_points.append({stop[1][\"stop_id\"]: timedelta(hours=0)})\n",
    "    return stop_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Exploration Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_exploration(graph, start, goal):\n",
    "    cities = load_cities()\n",
    "    explored = []\n",
    "    queue = [[start]]\n",
    "\n",
    "    if start == goal:\n",
    "        print(\"Same Node\")\n",
    "        return\n",
    "    \n",
    "    valide_routes = []\n",
    "    while queue:\n",
    "        path = queue.pop(0)\n",
    "        node = path[-1]\n",
    "        \n",
    "        if node not in explored and node != goal:\n",
    "            node_id = list(node.keys())[0]\n",
    "            neighbours = graph[node_id]\n",
    "            duration = timedelta(hours=0)\n",
    "            for neighbour in neighbours:\n",
    "                if list(neighbour)[0] in cities:\n",
    "                    new_path = list(path)\n",
    "                    new_path.append(neighbour)\n",
    "                    queue.append(new_path)\n",
    "                    duration = duration + neighbour[list(neighbour)[0]]\n",
    "                    if list(neighbour)[0] == list(goal)[0]:\n",
    "                        if len(valide_routes) > 25:\n",
    "                            return valide_routes\n",
    "                        valide_routes.append({\"route\": new_path, \"duration\": duration})\n",
    "            explored.append(node)\n",
    "    return valide_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the **shortest route**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_route(routes):\n",
    "    shortest_duration = routes[0][\"duration\"]\n",
    "    shortest_route = routes[0]\n",
    "    for route in routes:\n",
    "        if route[\"duration\"] < shortest_duration:\n",
    "            shortest_duration = route[\"duration\"]\n",
    "            shortest_route = route\n",
    "    return shortest_route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gare de Paris-Montparnasse 1-2 -> Gare de Versailles-Chantiers -> Gare de Chartres -> Gare de Voves -> Gare de Auneau -> Gare de Paris-Austerlitz -> Gare de Aubrais-(les) -> Gare de Vierzon -> Gare de Bourges -> Gare de Nevers -> Gare de Moulins-sur-Allier -> Gare de Paray-le-Monial -> Gare de Montchanin -> Gare de Dijon-Ville -> Gare de Lyon-Part-Dieu -> Gare de Lyon-Perrache en 0:08:00\n"
     ]
    }
   ],
   "source": [
    "station1, station2 = get_closest_stations(journey)\n",
    "\n",
    "if not station1 and not station2:\n",
    "        result = \"Trajet Impossible\"\n",
    "else:\n",
    "    graph = load_graph()\n",
    "    routes = graph_exploration(graph, station1, station2)\n",
    "    route = get_shortest_route(routes)\n",
    "    result = \"{} en {}\".format(\" -> \".join(convert_route_to_cities(route)), route[\"duration\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
